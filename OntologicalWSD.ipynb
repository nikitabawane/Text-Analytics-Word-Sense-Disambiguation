{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "q3QpGCyTkCkI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/rjain/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/rjain/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## IDS566: For Homework 2 Question 3\n",
    "# Submitted by : Nikita Bawane, Ritu Gangwal 670646774, Utkarsh Ujwal\n",
    "##\n",
    "\n",
    "# Importing all necessary files\n",
    "from lxml import etree, objectify\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import pandas as pd \n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "D3qC0trqkCkO"
   },
   "outputs": [],
   "source": [
    "# function to get sense from original dictionary: \n",
    "def getSenses(word, pos):\n",
    "    #global Tree\n",
    "    item = Tree.xpath(\"//lexelt[@item='%s.%s']\" % (word, pos))    \n",
    "    senses = []\n",
    "    if len(item) >= 1:\n",
    "        for sense in item[0].getchildren():\n",
    "            senses.append(dict(zip(sense.keys(), sense.values())))\n",
    "    return senses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the new dictionary by adding training data for corpus lesk\n",
    "def newDictionary():\n",
    "    parser = objectify.makeparser(recover=True)\n",
    "    tree = objectify.fromstring(''.join(open('dictionary.xml').readlines()), parser)\n",
    "    train_data_new = rename_columns(train_data)\n",
    "    for index, row in train_data_new.iterrows():\n",
    "        target_word = row['Target_Word'].strip()\n",
    "        sense_id = str(row['Sense_ID'])\n",
    "        sentence_to_add = row['Sentence']\n",
    "        \n",
    "        item = tree.xpath(\"//lexelt[@item='%s']\" % (target_word))\n",
    "        \n",
    "        for item_sense in item[0].getchildren():\n",
    "            if (str(item_sense.attrib['id']) == sense_id):\n",
    "                item_sense.attrib['examples'] = item_sense.attrib['examples'] + sentence_to_add\n",
    "\n",
    "    xml_new = etree.tostring(tree, pretty_print=True)\n",
    "    # save your xml\n",
    "    with open(r\"C:\\Users\\ritu2\\Desktop\\UIC MSBA\\Sem 2\\Text Analytics\\Assignments\\Assignment 2\\new_dictionary.xml\", \"wb\") as f:\n",
    "        f.write(xml_new)\n",
    "\n",
    "\n",
    "# function to get sense from new dictionary\n",
    "def getNewSenses(word, pos):\n",
    "    #global TreeNew\n",
    "    item = TreeNew.xpath(\"//lexelt[@item='%s.%s']\" % (word, pos))    \n",
    "    senses = []\n",
    "    if len(item) >= 1:\n",
    "        for sense in item[0].getchildren():\n",
    "            senses.append(dict(zip(sense.keys(), sense.values())))\n",
    "    return senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dVzZ_siskCkO"
   },
   "outputs": [],
   "source": [
    "# functions to clean all the datasets\n",
    "#rename columns and ignore the index column\n",
    "def rename_columns(dataset):\n",
    "    dataset_new = dataset.rename(columns = {0:\"Target_Word\", 1:\"Sense_ID\", 2:\"Sentence\"})\n",
    "    dataset_new = dataset_new.reset_index(drop=True)\n",
    "    return dataset_new\n",
    "\n",
    "\n",
    "#convert sentence column to lower case, remove digits and punctuations\n",
    "def lowercase_cleaned_data(dataset, colname):\n",
    "    stop = stopwords.words('english')\n",
    "    string.punctuation = string.punctuation.replace('%', '')\n",
    "    dataset[\"lowercase_cleaned\"] = dataset[colname].apply(lambda words: ' '.join(word.lower().translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) for word in words.split()))\n",
    "    dataset[\"lowercase_cleaned\"] = dataset[\"lowercase_cleaned\"].str.replace('\\d+', '')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "#retrieving pos for the words and lemmatisation\n",
    "def retreive_pos_wordnet(sentence):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    sentence = ' '.join(word.lower().translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) for word in sentence.split())\n",
    "    list_words = sentence.split()\n",
    "    final_list = []\n",
    "    for i in range (len(list_words)):\n",
    "        tag = nltk.pos_tag(list_words)[i][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        final_tag = tag_dict.get(tag, wordnet.NOUN)\n",
    "        lemmatized_word = lemmatizer.lemmatize(list_words[i],final_tag)\n",
    "        final_list.append([list_words[i],final_tag,lemmatized_word])\n",
    "    return final_list\n",
    "\n",
    "\n",
    "# function to remove stop words and words with length < 3\n",
    "def remove_stop_words_from_pos(pos_input_list):\n",
    "    return_list = []\n",
    "    stop = stopwords.words('english')\n",
    "    for pos in pos_input_list:\n",
    "        if (pos[2] not in stop and (len(pos[2])>2 or pos[2]==\"%%\")):\n",
    "            return_list.append(pos)\n",
    "    return return_list\n",
    "\n",
    "\n",
    "# function to clean dictionary\n",
    "def lemmatize_sentences(sentence):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    sentence = ' '.join(word.lower().translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) for word in sentence.split())\n",
    "    list_words = sentence.split()\n",
    "    lemmatize_words = ''\n",
    "    for i in range (len(list_words)):\n",
    "        tag = nltk.pos_tag(list_words)[i][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        final_tag = tag_dict.get(tag, wordnet.NOUN)\n",
    "        lemmatize_words += \" \" + lemmatizer.lemmatize(list_words[i],final_tag)   \n",
    "    return lemmatize_words.strip()\n",
    "\n",
    "def getLemmaExamplesFromSenseDict(word_pos, sense):\n",
    "    word_pos = word_pos.strip()\n",
    "    lemmaSenseKey = word_pos+ \"_\"+sense.get('id')\n",
    "    sense_examples = \"\"\n",
    "    if (lemmaSenseKey in SenseLemmaDictionary):\n",
    "        sense_examples = SenseLemmaDictionary.get(lemmaSenseKey)\n",
    "    else:\n",
    "        sense_examples = (\n",
    "            lemmatize_sentences(sense.get('gloss').lower())\n",
    "            + \" | \"\n",
    "            + ('.'.join(lemmatize_sentences(sentence.lower()) for sentence in sense.get('examples').split(\".\")))\n",
    "        )\n",
    "        SenseLemmaDictionary[lemmaSenseKey] = sense_examples\n",
    "    return sense_examples\n",
    "\n",
    "def getLemmaExamplesFromCorpusSenseDict(word_pos, sense):\n",
    "    word_pos = word_pos.strip()\n",
    "    lemmaSenseKey = word_pos+ \"_\"+sense.get('id')\n",
    "    sense_examples = \"\"\n",
    "    if (lemmaSenseKey in SenseLemmaCorpusDictionary):\n",
    "        sense_examples = SenseLemmaCorpusDictionary.get(lemmaSenseKey)\n",
    "    else:\n",
    "        sense_examples = (\n",
    "            lemmatize_sentences(sense.get('gloss').lower())\n",
    "            + \" | \"\n",
    "            + ('.'.join(lemmatize_sentences(sentence.lower()) for sentence in sense.get('examples').split(\".\")))\n",
    "        )\n",
    "        SenseLemmaCorpusDictionary[lemmaSenseKey] = sense_examples\n",
    "    return sense_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 : Simple lesk Algorithm\n",
    "def calculate_sense_model_one(target_word, pos_data):\n",
    "    print(target_word)\n",
    "    target_data = target_word.split(\".\")\n",
    "    senses = getSenses(target_data[0].strip(), target_data[1].strip())\n",
    "    score_map = {}\n",
    "    pos_sentence = []\n",
    "    for pos_word in pos_data:\n",
    "        pos_sentence.append(pos_word[2])\n",
    "    \n",
    "    for sense in senses:\n",
    "        sense_score = 0\n",
    "        sense_examples = getLemmaExamplesFromSenseDict(target_word, sense)\n",
    "        sense_example_words = sense_examples.split()\n",
    "        common = set(sense_example_words).intersection( set(pos_sentence) )\n",
    "        score_map[sense.get('id')] = len(common)\n",
    "    \n",
    "    key_max = max(score_map, key=score_map.get)\n",
    "    return key_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 : Orginal lesk Algorithm\n",
    "def limitizeContextMapModelTwo(context_sense):\n",
    "    dictionary_examples = \"\"\n",
    "    for context_data in context_sense:\n",
    "        for sense_data in context_data[2]:\n",
    "            #dictionary_examples += lemmatize_sentences(sense_data.get('gloss').lower())+ \" | \" + lemmatize_sentences(sense_data.get('examples').lower())\n",
    "            dictionary_examples += getLemmaExamplesFromSenseDict(context_data[1], sense_data)\n",
    "    return dictionary_examples\n",
    "\n",
    "\n",
    "def getContextDictModelTwo(target_data, pos_data, corpus=False):\n",
    "    context_sense = []\n",
    "    target_sense = []\n",
    "    sentence = pos_data\n",
    "    sentence_length = len(sentence)\n",
    "    target_word = target_data.split(\".\")[0]\n",
    "    target_pos = target_data.split(\".\")[1]\n",
    "    for k in range(len(sentence)):\n",
    "        if sentence[k][0] == \"%%\":\n",
    "            target_index = k-1\n",
    "            targetWord = sentence[target_index][0]\n",
    "            break\n",
    "    \n",
    "    i = target_index-2\n",
    "    j = target_index+2\n",
    "    k = 0\n",
    "    while((i>=0 or j<len(sentence)) and k<30):   \n",
    "        if(i>=0 and len(sentence[i][2].strip())>= 3 and sentence[i][2].strip() != target_word):\n",
    "            context_word = sentence[i][2].strip()\n",
    "            context_pos = sentence[i][1].strip()\n",
    "            if(corpus):\n",
    "                sense = getNewSenses(context_word,context_pos)\n",
    "            else:\n",
    "                sense = getSenses(context_word,context_pos)\n",
    "            \n",
    "            if len(sense) >= 1:\n",
    "                context_sense.append([targetWord,context_word+\".\"+context_pos,sense, target_index-i])\n",
    "            \n",
    "        if(j<len(sentence) and len(sentence[j][2].strip())>= 3 and sentence[j][2].strip() != target_word):\n",
    "            context_word = sentence[j][2].strip()\n",
    "            context_pos = sentence[j][1].strip()\n",
    "            if(corpus):\n",
    "                sense = getNewSenses(context_word,context_pos)\n",
    "            else:\n",
    "                sense = getSenses(context_word,context_pos)\n",
    "                \n",
    "            if len(sense) >= 1:\n",
    "                context_sense.append([target_word,context_word+\".\"+context_pos,sense, j-target_index])\n",
    "            \n",
    "        i = i-1\n",
    "        j = j+1\n",
    "        k = k+1\n",
    "                \n",
    "    return context_sense\n",
    "\n",
    "\n",
    "def calculateSenseIdModelTwo(target_word_pos, pos_without_stopwords):\n",
    "    print(target_word_pos)\n",
    "    target_word_details = target_word_pos.split(\".\")\n",
    "    target_senses = getSenses(target_word_details[0].strip(), target_word_details[1].strip())\n",
    "    score_map = {}\n",
    "    context_sentence = limitizeContextMapModelTwo(getContextDictModelTwo(target_word_pos, pos_without_stopwords))\n",
    "    \n",
    "    for sense in target_senses:\n",
    "        #sense_examples = lemmatize_sentences(sense.get('gloss').lower())+ \" | \" + lemmatize_sentences(sense.get('examples').lower())\n",
    "        sense_examples = getLemmaExamplesFromSenseDict(target_word_pos.strip(), sense)\n",
    "        sense_example_words = sense_examples.split()\n",
    "        context_example_words = context_sentence.split()\n",
    "\n",
    "        common = set(sense_example_words).intersection( set(context_example_words) )\n",
    "        context_score = len(common)\n",
    "        score_map[sense.get('id')] = context_score\n",
    "    \n",
    "    key_max = max(score_map, key=score_map.get)\n",
    "    return key_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 + 5 : Advance original lesk Algorithm with and without corpus lesk\n",
    "def getContextClassificationModel3(context_sense, corpus=False):\n",
    "    context_classification = {}\n",
    "    for context_data in context_sense:\n",
    "        dictionary_examples = \"\"\n",
    "        context_interval = int(context_data[3]/5)\n",
    "        for sense_data in context_data[2]:\n",
    "            if (corpus):\n",
    "                dictionary_examples += getLemmaExamplesFromCorpusSenseDict(context_data[1], sense_data)\n",
    "            else:\n",
    "                dictionary_examples += getLemmaExamplesFromSenseDict(context_data[1], sense_data)\n",
    "    \n",
    "        if(context_interval in context_classification):\n",
    "            dictionary_examples = context_classification.get(context_interval) + dictionary_examples\n",
    "        context_classification[context_interval] = dictionary_examples\n",
    "    return context_classification\n",
    "\n",
    "\n",
    "def calculateSenseIdModel3(target_word_pos, pos_without_stopwords, corpus=False):\n",
    "    print(target_word_pos)\n",
    "    target_word_details = target_word_pos.split(\".\")\n",
    "    \n",
    "    if(corpus):\n",
    "        target_senses = getNewSenses(target_word_details[0].strip(), target_word_details[1].strip())\n",
    "    else:\n",
    "        target_senses = getSenses(target_word_details[0].strip(), target_word_details[1].strip())\n",
    "    \n",
    "    score_map = {}\n",
    "    context_classification = getContextClassificationModel3(getContextDictModelTwo(target_word_pos, pos_without_stopwords, corpus), corpus)\n",
    "    \n",
    "    for sense in target_senses:\n",
    "        sense_score = 0\n",
    "        if (corpus):\n",
    "            sense_examples = getLemmaExamplesFromCorpusSenseDict(target_word_pos.strip(), sense)\n",
    "        else:\n",
    "            sense_examples = getLemmaExamplesFromSenseDict(target_word_pos.strip(), sense)\n",
    "        \n",
    "        for context_level in context_classification:\n",
    "            sense_example_words = sense_examples.split()\n",
    "            context_example_words = context_classification.get(context_level).split()\n",
    "            common = set(sense_example_words).intersection( set(context_example_words) )\n",
    "            context_score = len(common)*(6-int(context_level)+1)\n",
    "            sense_score += context_score\n",
    "        score_map[sense.get('id')] = sense_score\n",
    "    \n",
    "    key_max = max(score_map, key=score_map.get)\n",
    "    return key_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: Corpus lesk using simple algorithm\n",
    "def calculate_sense_corpus_model_one(target_word, pos_data):\n",
    "    print(target_word)\n",
    "    target_data = target_word.split(\".\")\n",
    "    senses = getNewSenses(target_data[0].strip(), target_data[1].strip())\n",
    "    score_map = {}\n",
    "    pos_sentence = []\n",
    "    for pos_word in pos_data:\n",
    "        pos_sentence.append(pos_word[2])\n",
    "   \n",
    "    for sense in senses:\n",
    "        sense_score = 0\n",
    "        sense_examples = getLemmaExamplesFromCorpusSenseDict(target_word, sense)\n",
    "        sense_example_words = sense_examples.split()\n",
    "        common = set(sense_example_words).intersection( set(pos_sentence) )\n",
    "        score_map[sense.get('id')] = len(common)\n",
    "   \n",
    "    key_max = max(score_map, key=score_map.get)\n",
    "    return key_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating accuracies of all models\n",
    "def calculate_accuracy(dataframe, column_name):\n",
    "    accuracy_number = 0\n",
    "    i=0\n",
    "    for index, row in dataframe.iterrows():\n",
    "        if(int(row['Sense_ID'])==int(row[column_name])):\n",
    "            accuracy_number += 1\n",
    "        i += 1\n",
    "    return ((accuracy_number/i)*100)\n",
    "\n",
    "# Exporting to CSV\n",
    "def exportToCSV(input_data_frame, csv_path):\n",
    "    tmp_df = input_data_frame.drop(['Sentence', 'lowercase_cleaned', 'pos_data'], axis=1)\n",
    "    tmp_df.to_csv(csv_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    global Tree\n",
    "    global TreeNew\n",
    "    global SenseLemmaDictionary\n",
    "    global SenseLemmaCorpusDictionary\n",
    "    SenseLemmaDictionary = {}\n",
    "    SenseLemmaCorpusDictionary = {}\n",
    "    # Read the dictionary file - original \n",
    "    Parser = objectify.makeparser(recover=True)\n",
    "    Tree = objectify.fromstring(''.join(open('dictionary.xml').readlines()), Parser)\n",
    "    \n",
    "    #read test data\n",
    "    train_data = pd.read_csv (r'C:\\Users\\ritu2\\Desktop\\UIC MSBA\\Sem 2\\Text Analytics\\Assignments\\Assignment 2\\train.data',header=None,delimiter = \"|\")\n",
    "    test_data = pd.read_csv (r'C:\\Users\\ritu2\\Desktop\\UIC MSBA\\Sem 2\\Text Analytics\\Assignments\\Assignment 2\\test.data',header=None,delimiter = \"|\")\n",
    "    validation_data = pd.read_csv (r'C:\\Users\\ritu2\\Desktop\\UIC MSBA\\Sem 2\\Text Analytics\\Assignments\\Assignment 2\\validate.data',header=None,delimiter = \"|\")\n",
    "\n",
    "    #rename columns for all the datasets\n",
    "    train_data_new = rename_columns(train_data)\n",
    "    test_data_new = rename_columns(test_data)\n",
    "    validation_data_new = rename_columns(validation_data)\n",
    "    \n",
    "    #create new dictionary\n",
    "    newDictionary()\n",
    "    ParserNew = objectify.makeparser(recover=True)\n",
    "    TreeNew = objectify.fromstring(''.join(open('new_dictionary.xml').readlines()), ParserNew)\n",
    "    \n",
    "    ################################# Validation data ###################################\n",
    "    # validation set cleaning process\n",
    "    method_one_validation_df = validation_data_new\n",
    "    method_one_validation_df = lowercase_cleaned_data(method_one_validation_df, 'Sentence')\n",
    "    method_one_validation_df[\"pos_data\"] = method_one_validation_df['lowercase_cleaned'].apply(lambda sentence: retreive_pos_wordnet(sentence))\n",
    "    method_one_validation_df[\"pos_data\"] = method_one_validation_df[\"pos_data\"].apply(lambda pos_data_list: remove_stop_words_from_pos(pos_data_list))\n",
    "    \n",
    "    # Model 1 - Simple lesk\n",
    "    method_one_validation_df['simple_lesk_sense_id'] = method_one_validation_df.apply(lambda x: calculate_sense_model_one(x['Target_Word'], x['pos_data']), axis=1)\n",
    "    \n",
    "    # Model 2 - Original Lesk\n",
    "    method_two_validation_df = method_one_validation_df\n",
    "    method_two_validation_df['original_lesk_sense_id'] = method_two_validation_df.apply(lambda x: calculateSenseIdModelTwo(x['Target_Word'], x['pos_data']), axis=1)\n",
    "    \n",
    "    # Model 3 - Advance original lesk\n",
    "    method_three_validation_df = method_two_validation_df\n",
    "    method_three_validation_df['adv_original_lesk_sense_id'] = method_three_validation_df.apply(lambda x: calculateSenseIdModel3(x['Target_Word'], x['pos_data']), axis=1)\n",
    "    \n",
    "    # Model 4 - Corpus lesk\n",
    "    method_four_validation_df = method_three_validation_df\n",
    "    method_four_validation_df['corpus_lesk_sense_id'] = method_four_validation_df.apply(lambda x: calculate_sense_corpus_model_one(x['Target_Word'], x['pos_data']), axis=1)\n",
    "    \n",
    "    # Model 5 - Adv Corpus lesk\n",
    "    method_five_validation_df = method_four_validation_df\n",
    "    method_five_validation_df['adv_corpus_lesk_sense_id'] = method_five_validation_df.apply(lambda x: calculateSenseIdModel3(x['Target_Word'], x['pos_data'], True), axis=1)\n",
    "    \n",
    "    \n",
    "    print(\"Accuracy of validation data for simple_lesk: \" + str(calculate_accuracy(method_one_validation_df, \"simple_lesk_sense_id\")))\n",
    "    print(\"Accuracy of validation data for original_lesk: \" + str(calculate_accuracy(method_two_validation_df, \"original_lesk_sense_id\")))\n",
    "    print(\"Accuracy of validation data for adv_original_lesk: \" + str(calculate_accuracy(method_three_validation_df, \"adv_original_lesk_sense_id\")))\n",
    "    print(\"Accuracy of validation data for corpus_lesk: \" + str(calculate_accuracy(method_four_validation_df, \"corpus_lesk_sense_id\")))\n",
    "    print(\"Accuracy of validation data for adv_corpus_lesk: \" + str(calculate_accuracy(method_five_validation_df, \"adv_corpus_lesk_sense_id\")))\n",
    "    \n",
    "    # Export validation results to CSV\n",
    "    exportToCSV(method_five_validation_df, r'C:\\Users\\ritu2\\Desktop\\UIC MSBA\\Sem 2\\Text Analytics\\Assignments\\Assignment 2\\validation_results.csv')\n",
    "    \n",
    "    ################################### Test data ######################################\n",
    "    \n",
    "    # test set cleaning process\n",
    "    method_one_test_df = test_data_new\n",
    "    method_one_test_df = lowercase_cleaned_data(method_one_test_df, 'Sentence')\n",
    "    method_one_test_df[\"pos_data\"] = method_one_test_df['lowercase_cleaned'].apply(lambda sentence: retreive_pos_wordnet(sentence))\n",
    "    method_one_test_df[\"pos_data\"] = method_one_test_df[\"pos_data\"].apply(lambda pos_data_list: remove_stop_words_from_pos(pos_data_list))\n",
    "    \n",
    "    # Model 1 - Simple Lesk\n",
    "    method_one_test_df['simple_lesk_sense_id'] = method_one_test_df.apply(lambda x: calculate_sense_model_one(x['Target_Word'], x['pos_data']), axis=1)\n",
    "    \n",
    "    # Model 2 - Original Lesk\n",
    "    method_two_test_df = method_one_test_df\n",
    "    method_two_test_df['original_lesk_sense_id'] = method_two_test_df.apply(lambda x: calculateSenseIdModelTwo(x['Target_Word'], x['pos_data']), axis=1)\n",
    "    \n",
    "    # Model 3 - Advance original lesk\n",
    "    method_three_test_df = method_two_test_df\n",
    "    method_three_test_df['adv_original_lesk_sense_id'] = method_three_test_df.apply(lambda x: calculateSenseIdModel3(x['Target_Word'], x['pos_data']), axis=1)\n",
    "    \n",
    "    # Model 4 - Corpus lesk\n",
    "    method_four_test_df = method_three_test_df\n",
    "    method_four_test_df['corpus_lesk_sense_id'] = method_four_test_df.apply(lambda x: calculate_sense_corpus_model_one(x['Target_Word'], x['pos_data']), axis=1)\n",
    "    \n",
    "    # Model 5 - Adv Corpus lesk\n",
    "    method_five_test_df = method_four_test_df\n",
    "    method_five_test_df['adv_corpus_lesk_sense_id'] = method_five_test_df.apply(lambda x: calculateSenseIdModel3(x['Target_Word'], x['pos_data'], True), axis=1)\n",
    "    \n",
    "    # Export validation results to CSV\n",
    "    exportToCSV(method_five_test_df, r'C:\\Users\\ritu2\\Desktop\\UIC MSBA\\Sem 2\\Text Analytics\\Assignments\\Assignment 2\\test_data_results.csv')\n",
    "    \n",
    "    ################################### Training data ######################################\n",
    "    \n",
    "    # train data cleaning process\n",
    "    method_one_train_df = train_data_new\n",
    "    method_one_train_df = lowercase_cleaned_data(method_one_train_df, 'Sentence')\n",
    "    method_one_train_df[\"pos_data\"] = method_one_train_df['lowercase_cleaned'].apply(lambda sentence: retreive_pos_wordnet(sentence))\n",
    "    method_one_train_df[\"pos_data\"] = method_one_train_df[\"pos_data\"].apply(lambda pos_data_list: remove_stop_words_from_pos(pos_data_list))\n",
    "    \n",
    "    # Model 1 - Simple Lesk\n",
    "    method_one_train_df['simple_lesk_sense_id'] = method_one_train_df.apply(lambda x: calculate_sense_model_one(x['Target_Word'], x['pos_data']), axis=1)\n",
    "    \n",
    "    # Model 2 - Original Lesk\n",
    "    method_two_train_df = method_one_train_df\n",
    "    method_two_train_df['original_lesk_sense_id'] = method_two_train_df.apply(lambda x: calculateSenseIdModelTwo(x['Target_Word'], x['pos_data']), axis=1)\n",
    "    \n",
    "    # Model 3 - Advance original lesk\n",
    "    method_three_train_df = method_two_train_df\n",
    "    method_three_train_df['adv_original_lesk_sense_id'] = method_three_train_df.apply(lambda x: calculateSenseIdModel3(x['Target_Word'], x['pos_data']), axis=1)\n",
    "    \n",
    "    # Calculating accuracies of training data\n",
    "    print(\"Accuracy of training data for simple_lesk: \" + str(calculate_accuracy(method_one_train_df, \"simple_lesk_sense_id\")))\n",
    "    print(\"Accuracy of training data for original_lesk: \" + str(calculate_accuracy(method_two_train_df, \"original_lesk_sense_id\")))\n",
    "    print(\"Accuracy of training data for adv_original_lesk: \" + str(calculate_accuracy(method_three_train_df, \"adv_original_lesk_sense_id\")))\n",
    "                                                                     \n",
    "                                                                     \n",
    "    # Export validation results to CSV\n",
    "    exportToCSV(method_three_train_df, r'C:\\Users\\ritu2\\Desktop\\UIC MSBA\\Sem 2\\Text Analytics\\Assignments\\Assignment 2\\training_data_results.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW2Code_V01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
